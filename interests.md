---
layout: page
title: Interests
permalink: /interests/
published: false
---

The current questions that interest me. This page will (hopefully) be relatively dynamic over time.



### Efficient learning

> Intelligence is the ability to solve problems efficiently (?)

How fast is it possible to learn something? (lower bounds - just assume everything…) How much do you have to search to find the best solution?  Active learning. Building the perfect scientist. Querying the world, falsifying hypotheses. Prediction and action.

Low-power learning, lazy evaluation, ‘just in time’ processing. Outsourcing memory to the environment, and predicting informations availability.

How can you implement learning to learn? How can you learn to learn faster? Learn a convolution?

Compression as intelligence and consciousness. AIT.  Efficiency.  Compressing memories. RNNs as program compressors?

Attention!!


### Learning styles

How can you augment an existing learning system to make is smarter? integrating with existing networks/systems. Transfer learning ... Continuous/on-line learning.

How to teach? Curriculum learning? Bootstrapping loss functions, … ???

Model based learning. Teaching processes. Learning/testing/correcting a part. Causality??

A way to understand the function of complex networks. To know how to networks are similar, what they share, how to morph one into the other, what structure one needs to do X function, ...

How can we learn good representations? How do we/nns understand things. Invariance and atomic. Universal languages. Semantics and grammar. How can we represent things in a way that allows us to do/understand (applied to people and computers).

Constraining learning, weight tying, information propagation and types, higher-level structures.

Self-supervision and context.

RL and unsupervised. Obviously…

### Distributed systems

How can we coordinate multiple learners/agents? 
What happens if they process things at different speeds, how can we assign credit? Timing, ... Causality.  
Is there a language we can invent for thinking about systems of loss functions, where they can be composed and decomposed? (is the related to dynamic programming?)


Can we do as well or better than backprop through time (for training recurrent nets) with an efficient online algorithm (not requiring to store all our mental states for our lifetime and then play it backwards)?  Brains clearly achieve this feat but we have no clue how. TD learning? 

### Maths

Gradients are the reason we can optimise these things. They are the reason we are not doing a blind random search (like evolutionary methods). I should get to know these.

Computational graphs and how to propagate information. Propagating gradients (automatic differentiation and gradient estimation) and uncertainty (???).

Functional programming ???

What is the relation between structure/nature/functional and optimisation?

Optimisation. The search for ?? exploration vs exploitation, … ???

Memory. How do we decide what to remember? How can we store it efficiently, by picking out patterns, easy retrieval, ...

### Other neural net stuff

How can we compose and manage neural networks?What if I want to make a net to make a net?

Replace everything with neural networks. Adam, don’t need that, let a NN do it. 
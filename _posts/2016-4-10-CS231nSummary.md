
# Intro

So, I (very loosely) followed the stanford course on convolutional neural networks (CS231n). First I tried (and failed) to build a conv net by hand, implementing everything from scratch. I then moved on (gave up) to implementing a conv net in tensorflow and tried to reach state-of-the-art accuracy on CIFAR10.

# Hand built

> It turns out, implementing backprop in convolutional neural networks is not trivial. 

I started out writing n-dimensional recursive convolution algorithm, which was quite beautiful if you ask me. However, when I got to implementing backprop it just proved to be too confusing (as recursive algorithms are harder to peer into). Lesson learnt: start simple.

* So in the first part of my work ([Image processing and kernels](https://github.com/act65/Uni/blob/master/CS231n/ImagesandCIFAR10.ipynb)) I explored how a kernel is convolved over an image and the properties of the CIFAR10 images.
* I then moved onto implementing [Conv, softmax, activation, norm layers](https://github.com/act65/Uni/blob/master/CS231n/LayersArchitecture.ipynb), in a nice general structure so they could be tacked together to form deep nets.
* Next I tried to sort out the [Loss and classification](https://github.com/act65/Uni/blob/master/CS231n/ClassificationLossandOptimisation.ipynb). I have a little think about what a loss function should do, but don't consider it too deeply.
* And lastly I implemented [Gradient checking](https://github.com/act65/Uni/blob/master/CS231n/AnalysisTools.ipynb) (dont look at this one...).

The places where I got stuck were;

* on softmax. My math needs some improvement as I struggled through the derivation.
* understanding how 


# Benchmark

I went into this after reading [Striving for simplicity](http://arxiv.org/abs/1412.6806) and was keen to go as simple as possible. I wanted a skinny minimally deep network that would learn from looking at the data once.

What I learnt was: if your network has the perfect capacity for the problem (aka it's small), it probably wont work. You still have to find a good minima. That is why we make oversized networks and regularise them, to ensure that there are many good minima that we can fall into.

So I made a 12 layer network in tensorflow ([here](https://github.com/act65/Uni/blob/master/CS231n/DeepNet%20in%20TF.ipynb)) with convolutions instead of pooling overations and only 30 nodes wide. And got modest (to poor) accuracy (around 65%). However, I think this could be inproved significantly if I had the patience to train on more epochs (in the hundreds) and to add a regulaiser (such as dropout) to stop the resulting overfitting.

# Interesting points

> Make sure that you can overfit very small portion of the training data. (CS231n)

* By convolving a single neuron (with the same weights) over all the x,y positions of an image we reduce the number of parameters required to learn. Also, it makes the features the neuron learns invariant to changes in x and y. 
    * Interesting work could (and is) being done to extend this idea of invariance to different vairables, e.g. rotation.


# Unanswered questions

* How does batch size effect learning?
* I reckon there is a way to get sinusoidal activation functions working effectively in these networks. The idea being that it can add complexity (through information entanglement) to smaller networks, in some sense boosting (but not that boosting) their capacity.
* How does it makes sense (in dropout) to average weight changes across different (similar) models?
* All kernels connected to softmax. Seems to do worse...

# Resources and references

## Websites

* [Andrej Karpathy](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)
* [Stanford course - CS231n](http://cs231n.stanford.edu/)
* [Tensor flow tutorial](https://www.tensorflow.org/versions/r0.7/tutorials/deep_cnn/index.html)
* [Image convolution demo](https://tonysyu.github.io/ipython-jupyter-widgets-an-image-convolution-demo.html#.VtngZZN96zY)
* [Best computer vision results](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#53544c2d3130)
* [Cross entropy for classification](https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/)
* [Softmax derivation](http://stats.stackexchange.com/questions/79454/softmax-layer-in-a-neural-network)

## Papers

* [Striving for simplicity](http://arxiv.org/abs/1412.6806)
* [All you need is a good init](http://arxiv.org/abs/1511.06422)
* [Do deep nets really need to be deep?](http://arxiv.org/pdf/1312.6184.pdf)
* [ImageNet classification with deep convolutional neural networks](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)
* [Batch normalisation](http://arxiv.org/abs/1502.03167)
* [Dropout](http://www.cs.toronto.edu/~fritz/absps/dropout.pdf)

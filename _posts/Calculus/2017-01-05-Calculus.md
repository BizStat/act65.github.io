---
layout: post
title: Calculus
category: main
published: false
---

[The essence of calculus]({{ baseurl }}/EssenceofCalc)
[Motivating calculus]({{ baseurl }}/MotivationofCalc)

<<<<<<< HEAD
Decomposing hard problems into an approximation via a large sums of distinct parts. But can then efficiently evaluate the approximation by taking a limit.
=======
Linearization!!
Continuious!!
>>>>>>> b00124d1f9902c5f01aaf96a9883ad5393827b22


## Differences

### Finite differences

Want to

Alternative derivation/framing?

Connected neighbors. Approximation given local behaviour.

[intuition]({{ baseurl }}/)

which leads to empirical calculation of gradients.


### Temporal differences.

Credit assignment in ML.
How is it used. What does it do. Local credit assignment.
Making it efficient? AD?

## Automatic differentiation

Forward and reverse AD.
Relation to symbolic gradients.
Relation to duals.([doubly recursive gist]({{ gist }}/recursiveAD) and )

[minimal implementation of reverse AD on graphs]({{ gist }}/simpleAD)
Incremental lambda calculus.

## Non-differentiable

### Solutions. GA, RL.

Problem Credit assignment

### Settings

* discrete (soft approximations)
* random (mean field).
* unknown (Balduzzi on approximation gradient fns)



solutions?

concrete distribution. ...
MCMC of counterfactuals?

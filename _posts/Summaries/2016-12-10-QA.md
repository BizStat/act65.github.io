---
layout: post
title: Learning to Compose Neural Networks for Question Answering
category: summary
---

_This post is based around [Neural Module Networks](https://arxiv.org/pdf/1511.02799v3.pdf) and [Learning to Compose Neural Networks for Question Answering](https://arxiv.org/abs/1601.01705) by some of the CS guys at Berkeley._


### Problem setting

We want to answer a question about some other object (an image, a database, a text document). The question is received as natural language (represented as a string) and the answer is expected in natural language (also as a string). 

High level solutions to this problem can also be framed as;

* information retrieval. All the information we want is (or is not) contained in the object we are querying. We just need to know how to get it out.
* knowledge base querying.
* ??


Fundamentally it seems that we need to;

* parse the question,
* extract information from the object,
* ?

There are a few approaches to answering questions about an image. Which are … . A more interesting question, is whether you need the same amount of computation to answer each question. Are some questions harder to answer than others? How can we take advantage of this to be more efficient?

So we ??? is effectively learning to parse the question, and to map from the parsed question to a network layout.


### Why is this problem hard/useful?

Applications? And difficulty?
$\mathcal O(?)$

### How did they do it?

Definitions:

1. $w$, a world representation (e.g. if we are answering questions about images we might use a feature map, or the actual image??, from a CNN)
2. $x$, a question (represented as a string of words, as a one hot sequence of words??)
3. $y$, an answer (a distribution over words)
4. $z$, a network layout (represented as ??)


Split answering into three tasks:

* $p(z \mid x)$, a distribution over likely parses (different ways to interpret/answer the question) (translating from their language to yours)
* $p_z(y \mid w)$, a distribution over likely answers given an architecture (query the input/object for the answer)
* $v(? \mid z, w)$, a value function for ‘`good`’ architectures (evaluate …)


With a series of modules of two types.

> <u>Attention</u> is a distribution over pixels or entities and <u>Labels</u> is a distribution over answers.


So the semantics of our language are ??? and the syntax is … defined by the types.

* `find[i]`
* ..

### What problem does it solve?

There seem to be two main approaches that (to some extent) work.

1. semantic parsers that decompose questions into logical expressions.
2. differentiable classifiers that filter information from the object through the lens of the question

This composition can be interpreted as gating [1](http://papers.nips.cc/paper/6261-visual-question-answering-with-question-representation-update-qru.pdf), predicting parameters or bilinear tensor operations [2](https://arxiv.org/abs/1606.01847).

The problem with this work is that ??? Therefore …



### Thoughts

(what does p(z \mid x) need to know? How does module f_i act. How does it effect a larger system when in position x. How do different modules interact. … We want a cheaper approximation that we can use to predict which graphs would be useful)


How is the problem best framed? Querying an object, versus gating information versus ???
To accurately and efficiently extract the information we want it seems intuitive that we need to know what we are looking for. However, it is not necessarily that simple. Ambiguity in the question may only become resolved after observing the context that it is meant.

> Rather than thinking of question answering as a problem of learning a single function to map from questions and images to answers, it is perhaps useful to think of it as a highly-multitask learning setting, where each problem instance is associated with a novel task, and the identity of that task is expressed only noisily in language.

Language is a tree/graph. If we ever want to be able to … we need to match its structure.

Common question in language design. What are the primitive functions required for universal computation. And what are the primitives suited to this application to make it efficient?
![relate]({{ baseurl }}/images/relate.png){: .center-image }


### Why I think this paper is important

The title of section 2 pretty much sums up my interest in it: __Deep networks as functional programs__. This idea of differentiable programming has been floating around for a while, and there doesn’t seem to have been much significant progress on it (some links to related work?).

Since I read [Colah’s](http://colah.github.io/posts/2015-09-NN-Types-FP/) post on NNs and functional programming I have had the idea of .

Higher-level networks that can pass around other networks. Or in other words, we have first-class neural functions. I think this is what reasoning is, the ability to compose …

Interestingly, there are a couple of high level approaches to composing NNs. Distributed vs centralised.

And. If you ask me, reasoning and being able to compose … is …

> Rather than using logic to reason over truth values, the representations computed by our model remain entirely in the domain of visual features and attentions.

We are reasoning (composing …???) with learned visual and linguistic representations!


More weight tying. Reasoning solves a concrete computational problem. There is structure in each question. By using this we can learn faster and more accurately, and ??? (a name for carving nature at its joints)

Modular knowledge is transferable!

### Issues, criticisms and things to keep in mind

One word answers make things a lot easier.

Solving the problem of localisation/grounding. Softmax over pixels is probably a bad idea. Alternatives are ??, ??. [ref](https://arxiv.org/pdf/1511.03745v3.pdf)

Every module has access to the world state, $w$. So $\tilde w(h) = \sum_k h_kw_k$ is an expensive computation.

> These symbolic representations already determine the structure of the predicted networks, but not the identities of the modules that compose them. This final assignment of modules is fully determined by the structure of the parse.

Batch learning. And scalability.

> Networks which have the same high-level structure but different instantiations of individual modules (for example what color is the cat? / describe[color](find[cat]) and where is the truck? / describe[where](find[truck])) can be processed in the same batch, allowing efficient computation.



### Questions


* Type constraints. How do they enforce these?
* Parameter prediction is just a multiplicative connection?
* relation to parsing?
* relation to reasoning?
* relation to multimodal language translation?
What other functions/modules would we want?
What other types make sense?
* How is the distribution over pixels actually used?? Where do the features come from? Are they gated by this distribution?
* Where are the parameter arguments coming from? Continuous representations of a word/sentence? How do we know which word to `find[i]`?
* What happens if you remove some of the strict constraints on possible architectures? Having two `relate` fns, or ??.
* it feels like there should be more types/intermediate representations. What would they be. And/or how could we learn these? But what structure should they have and how can we regularise that into them?
* Learning to communicate. Each different protocol is a type? Learning to communicate is the same as learning a type system?

### Future work

* Would like a proof, which would be rather simple. That composing modules for question answering, is;
	* more efficient for answering questions. (requires less … memory, …
	* is easier to learn independent …s
* I am sure that there is a nice representation of graphs that will allow the backdrop of gradients.  
* [differentiable graphs](https://tkipf.github.io/graph-convolutional-networks/). Although, more flexible languages to specify architectures means they will be harder to learn. A question I am interested in is: _How can we bias the architectures generated through the language we choose to represent them with?_ How is this related https://arxiv.org/pdf/1609.05600.pdf
* Interested to watch progress on [spatial-temporal](https://arxiv.org/abs/1612.01669) answering and [question asking](https://arxiv.org/abs/1611.08481)
* Relation to learning to communicate? _(Depending on the underlying structure, these messages passed between modules may be raw image features, attentions, or classification decisions; each module maps from specific input to output types.)_
* Less rigid/specified modules. Want to learn them.
* Local attention. Or some hierarchical representation of attention. As that shit is expensive.

##### Problems to be solved

* Map what the question refers to within the object. (aka grounding of verbs and nouns??)
* ??
* ??

> the more general and challenging task of localizing entities based on arbitrary natural language expressions remains far from solved. ([Hu et al. 2016](https://arxiv.org/abs/1611.09978))

(can this be done using a generative model? why would you want to? . Generate an image from the language expression and minimise error from what you are seeing?)



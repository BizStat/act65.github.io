---
layout: post
title: Can PID control be used on gradient descent?
category: problem
published: false
---

Is there a way to use methods from control theory to aid the optimisation of neural networks? Specifically, I feel like PID control may have some useful things to say.

Typically PID control gives $u(t)=K_p e(t)+K_d{\frac {de(t)}{dt}}+K_i\int_0^te(t)\,dt$ where $u(t)$ is the variable to be controlled and $e(t)$ is some measure of error. Can we adapt this to control NNs, where $u(t)$ is the  parameters, and our measure of error, $e(t)$, is the loss.

$$\theta^{t+1} = K_p\frac{\partial \mathcal L}{\partial \theta^t} + \sum_{i=0}^t K_d^t (\frac{\partial \mathcal L}{\partial \theta^t} -\frac{\partial \mathcal L}{\partial \theta^{t-1}}) + \sum_{i=0}^t K_I^t\frac{\partial \mathcal L}{\partial \theta^t}$$


***

UPDATE:


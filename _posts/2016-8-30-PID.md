---
layout: post
title: Can PID control be used on gradient descent?
category: problem
---

Is there a way to use methods from control theory to aid the optimisation of neural networks? Specifically, I feel like PID control may have some useful things to say.

Typically PID control gives $u(t)=K_P e(t)+K_I\int_0^te(t)\,dt +K_D{\frac {de(t)}{dt}}$ where $u(t)$ is the variable to be controlled and $e(t)$ is some measure of error. Can we adapt this to control NNs, where $u(t)$ is the  parameters, and our measure of error, $e(t)$, is the loss or gradients of the loss.


$$\theta^{t+1} = 
K_P \theta^{t} + 
\sum_{i=0}^t K_I^t\Delta \theta^t +
\sum_{j=0}^t K_D^t\frac{\partial \mathcal L}{\partial \theta^t}$$

This is kind of nice as proportional control gives SGD, integral control gives momentum and derivative control gives and additive factor of second order gradients (does that make sense?).

Alternatively, we could use the gradient of the loss.

$$\theta^{t+1} = K_P\frac{\partial \mathcal L}{\partial \theta^t} + 
\sum_{i=0}^t K_I^t\frac{\partial \mathcal L}{\partial \theta^t} +
\sum_{j=0}^t K_D^t\frac{\partial^2 \mathcal L}{\partial {\theta^t}^2}$$

***

UPDATE:
This is looking less and less like a good idea. 
---
layout: post
title: Can PID control be used on gradient descent?
category: problem
---

Is there a way to use methods from control theory to aid the optimisation of neural networks? Specifically, I feel like PID control may have some useful things to say.

Typically PID control gives $u(t)=K_p e(t)+K_d{\frac {de(t)}{dt}}+K_i\int_0^te(t)\,dt$ where $u(t)$ is the variable to be controlled and $e(t)$ is some measure of error. Can we adapt this so that the variables to control is our parameters, and our measure of error is the gradients w.r.t. the loss.

$$\theta^{t+1} = K_p\frac{\partial \mathcal L}{\partial \theta^t} + \sum_{i=0}^t K_d^t (\frac{\partial \mathcal L}{\partial \theta^t} -\frac{\partial \mathcal L}{\partial \theta^{t-1}}) + \sum_{i=0}^t K_I^t\frac{\partial \mathcal L}{\partial \theta^t}$$

Notably _proportional control_ gives vanilla gradient descent. A finite difference of the estimate for _derivative control_ approximates the hessian (???). And _integral control_ gives momentum.


If momentum is the sum of past gradients, thus doing an integral of the gradients with steps of 1. 

$$ M[g^2]^t= \gamma M[g^2]^{t-1}+(1−\gamma){g^2}^t$$

How about taking the difference between gradients and using that to estimate the curvature?

$$ D[g^2]^t= \gamma D[g^2]^{t-1} - (1−\gamma){g^2}^t$$

### Links, thoughts and questions

* What does the differential term mean? It's a finite difference at different time steps. Does this approximate the hessian, or just the diagonals?
* 
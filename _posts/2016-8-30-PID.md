---
layout: post
title: PID control for gradient descent
category: problem
---

Is there a way to use methods from control theory to aid the optimisation of neural networks? Specifically, I feel like PID control may have some useful things to say.

$$\Delta \theta = k_P  \nabla_{\theta}\mathcal L+ k_I \int \Delta \theta + k_D \nabla^2_{\theta}\mathcal L$$

* _Proportional control_ is proportional to the gradient.
* _Integral control_ is proportional to the integral of our gradients. (aka momentum is a simple approximation)
* _Derivative control_ is proportional to the hessian, or curvature. (is there some simple approximation of this?)

If momentum is the sum of past gradients, thus doing an integral of the gradients with steps of 1. 

$$ M[g^2]^t= \gamma M[g^2]^{t-1}+(1−\gamma){g^2}^t$$

How about taking the difference between gradients and using that to estimate the curvature?

$$ D[g^2]^t= \gamma D[g^2]^{t-1} - (1−\gamma){g^2}^t$$